# latent_mathematical_reasoning

Data: https://drive.google.com/drive/folders/1M8XQOuzIL0GgRQjcnmtffUTX7qhWYuFk?usp=share_link

Results: https://docs.google.com/spreadsheets/d/1u81xRR2TSoWF1s0KlsL39ynImksZICKJcdZEIYqyTys/edit?usp=sharing 

Paper: https://www.overleaf.com/3768452489nkvcsvqkfggv 

![Image description](latent_math_reasoning.png)

## Related Work
- Mthematical Reasoning in Latent Space
- Symbolic Brittleness in Sequence Models: on Systematic Generalization in Symbolic Mathematics
- A Symbolic Framework for Systematic Evaluation of Mathematical Reasoning with Transformers
- HOList: An Environment for Machine Learning of Higher-Order Theorem Proving
- Latent Action Space for Efficient Planning in Theorem Proving
- Graph Representations for Higher-Order Logic and Theorem Proving
- Can Neural Networks Learn Symbolic Rewriting?
- MATHEMATICAL REASONING VIA SELF-SUPERVISED SKIP-TREE TRAINING
- Learning to Reason in Large Theories without Imitation
- ANALYSING MATHEMATICAL REASONING ABILITIES OF NEURAL MODELS
- Neural Machine Translation for Mathematical Formulae
- DEEP LEARNING FOR SYMBOLIC MATHEMATICS
- https://github.com/Jan21/symbols-and-patterns
- On the Paradox of Learning to Reason from Data
- A Survey of Deep Learning for Mathematical Reasoning (https://github.com/lupantech/dl4math)
- Systematic Generalization and Emergent Structures in Transformers Trained on Structured Tasks
- The Devil is in the Detail: Simple Tricks Improve Systematic Generalization of Transformers
- Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples
- A Causal Framework to Quantify the Robustness of Mathematical Reasoning with Language Models
- Large Language Models are In-Context Semantic Reasoners rather than Symbolic Reasoners
- LIME: Learning Inductive Bias for Primitives of Mathematical Reasoning
- https://openreview.net/forum?id=pbUcKxmiM54
- Faith and Fate: Limits of Transformers on Compositionality
- Deductive Additivity for Planning of Natural Language Proofs: https://github.com/Zayne-sprague/Deductive_Additivity_for_Planning_of_Natural_Language_Proofs
- Grokking of Hierarchical Structure in Vanilla Transformers
- Towards understanding grokking: An effective theory of representation learning
- Learning Neural PDE Solvers with Parameter-Guided Channel Attention
- How neural networks extrapolate: From feedforward to graph neural networks
- Teaching Arithmetic to Small Transformers
- Reasoning or Reciting? Exploring the Capabilities and Limitations of Language Models Through Counterfactual Tasks
- Generalization on the unseen, logic reasoning and degree curriculum
- Specializing Smaller Language Models towards Multi-Step Reasoning
- NEURAL NETWORKS AND THE CHOMSKY HIERARCHY
- Underspecification Presents Challenges for Credibility in Modern Machine Learning
